name: Market Analysis Pipeline

on:
  # Allows manual triggering from the Actions tab
  workflow_dispatch:
  # Runs automatically on a schedule (e.g., every Friday at 18:00 UTC)
  schedule:
    - cron: '0 18 * * 5'

jobs:
  analyze-and-build:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    # --- Setup Environment ---
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
        # If your scrapers require other dependencies (e.g., Playwright, BeautifulSoup), install them here too.
        # pip install playwright beautifulsoup4
        # playwright install

    # --- Data Acquisition (Scraping) ---
    - name: Run Scrapers and Populate Database
      run: |
        echo "Running data acquisition..."
        # CRITICAL: This is where your scrapers run to populate market_reports.db
        # According to the PSD, this is main_runner.py. Uncomment the next line when ready.
        # python main_runner.py
        echo "Scraping step finished."
        
        # Check if the database exists (either from Git or generated by scrapers)
        if [ ! -f market_reports.db ]; then
          echo "WARNING: market_reports.db not found. Analysis may fail if scrapers did not generate it."
          # Optional: Create an empty DB if missing, so analysis script doesn't crash on file-not-found.
          # touch market_reports.db
        fi

    # --- Data Analysis ---
    - name: Run Mombasa Analysis
      # This generates report_data/mombasa_index.json
      run: python analyze_mombasa.py
      # Add 'continue-on-error: true' if you want the workflow to proceed even if analysis fails
      # continue-on-error: true

    # As you add more analyzers, list them here:
    # - name: Run Colombo Analysis
    #   run: python analyze_colombo.py

    # --- Library Consolidation ---
    - name: Build Consolidated Market Library
      # This reads the intermediate indexes and generates market-reports-library.json
      run: python build_library.py

    # --- Commit and Push Changes ---
    - name: Commit and push generated files
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action Bot"

        # Add the generated data directory and the updated master library
        # Use '|| true' to prevent the step from failing if the directory/files don't exist
        git add report_data/ || true
        git add market-reports-library.json || true

        # Commit only if there are changes
        if [ -z "$(git status --porcelain)" ]; then
          echo "No changes to commit."
        else
          # [skip ci] prevents this commit from triggering another workflow run unnecessarily
          git commit -m "Automated Update: Regenerated analysis data and library index [skip ci]"
          git push
        fi